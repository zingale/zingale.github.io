<!DOCTYPE HTML>
<!--
    Halcyonic by HTML5 UP
    html5up.net | @ajlkn
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
  -->
<html>
  <head>
    <title>Michael Zingale: Algorithms</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="../../assets/css/main.css" />
  </head>
  <body class="subpage">
    <div id="page-wrapper">

      <!-- Header -->
      <section id="header">
	<div class="container">
	  <div class="row">
	    <div class="col-12">

	      <!-- Logo -->
	      <h1><a href="../../index.html" id="logo">Michael Zingale</a></h1>

	      <!-- Nav -->
	      <nav id="nav">
		<a href="../../index.html">Homepage</a>
		<a href="../../research.html">Research</a>
		<a href="../../pub_year.html">Papers</a>
		<a href="../../classes.html">Classes</a>
		<a href="../../codes.html">Codes</a>
	      </nav>

	    </div>
	  </div>
	</div>
      </section>

      <!-- Content -->
      <section id="content">
	<div class="container">
	  <div class="row">
	    <div class="col-4 col-12-medium">
              <div id="sidebar">

                <!-- Sidebar -->
                <section>
	          <ul>
		    <li><h4>Simulation methodology</h4>

                      <span>Our group develops several simulation codes
                        designed to model astrophysical flows, in
                        particular, the explosive environments found in
                        stellar explosions.
                        The <a href="#amrexastro">AMReX-Astrophysics
                          suite</a> of codes is built on the adaptive mesh
                        refinement library AMReX and designed to
                        efficiently model low Mach number convection and
                        highly-compressible flows in stars.  We pay
                        particular attention to
                        the <a href="#sdc">coupling of reactions and
                          hydrodynamics</a> to ensure that we accurately
                        model stellar environments where nucleosynthesis
                        takes place, and use <a href="#gpu">performance
                          portable</a> parallelization techniques to ensures
                        that we perform well at scale on modern
                        supercomputers.
                      </span>
                    </li>

                    <li><h4>Support</h4>
                      <span>Supported by the DOE Office of Nuclear Physics.</span></li>

		    <li><h4>Open Science</h4>
		      <span>All of our simulation codes are fully open source and
                        available on github</span> </li>
	          </ul>
	        </section>

              </div>   <!-- sidebar -->
	    </div>

	    <div class="col-8 col-12-medium imp-medium">

	      <!-- Main Content -->
	      <section>
		<header>
		  <h2>AMReX-Astrophysics</h2>
	        </header>

                <p><h3 class="subsection">Castro: compressible flows</h3>

                <p>Our group develops
                  the <a href="https://amrex-astro.github.io/Castro/">Castro</a>
                  compressible (magneto-, radiation) hydrodynamics code.
                  Castro supports a general equation of state, arbitrary
                  nuclear reaction network, full self gravity w/
                  isolated boundary conditions, thermal diffusion,
                  flux-limited diffusion (multigroup) radiation,
                  rotation, and more.

                <p>Castro runs on anything from laptops to
                  supercomputers, using MPI+OpenMP for CPUs and MPI+CUDA
                  for GPUs.

                <p><img class="center" src="slice_grid.png" alt="the adaptive mesh refinement grid in Castro">

                <p>We use Castro for our <a href="../wdmerger">white
                    dwarf merger</a> and <a href="../xrb">X-ray burst</a>
                  simulations.

                <p><h3 class="subsection">MAESTROeX: low Mach number stellar flows</h3>

                <p>We also develop (together with LBNL)
                  the <a href="https://amrex-astro.github.com/MAESTROeX">MAESTROeX</a>
                  low Mach number stellar hydrodynamics code.  MAESTROeX
                  filters soundwaves from the equations of hydrodynamics
                  while keeping compressibility effects due to
                  stratification and local heat release.  This enables
                  it to take large timesteps, not constrained by the
                  soundspeed, for subsonic flows.

                <p>We use MAESTROeX (and its predecessor MAESTRO) for
                  our <a href="../wdconvect">white dwarf
                    convection</a>, <a href="../xrb">X-ray burst</a>,
                  and <a href="../subchandra">sub-Chandra Type Ia
                    supernovae</a> simulations.


              </section>

              <section>
                <a name="sdc"></a>
	        <header class="major">
		  <h2>Reactive flows</h2>
	        </header>

                <p>All of our simulations involve reacting
                  flows&mdash;the immense energy release from nuclear
                  burning drives hydrodynamic flows.  These two
                  processes need to be tightly coupled together to
                  ensure that we accurately capture the dynamics and
                  nucleosynthesis.  The traditional method of coupling
                  hydrodynamics and reactions in astrophysics has been
                  Strang splitting, a type of operator splitting where
                  the reactions and hydrodynamics operations each act
                  on the state left behind from the other process, but
                  there is no explicit coupling.  We have been
                  developing spectral deferred correction (SDC)
                  techniques to strongly couple the two processes.  In
                  SDC methods, the hydrodynamics explicitly sees a
                  reaction source and the reactions take into account
                  how advection alters the state during the burn.
                  Iteration is used to fully couple the processes.
                  SDC methods are integrated into both Castro and
                  MAESTROeX.

                <p><img class="center" src="sdc_plot.png" alt="two timesteps comparing the SDC and Strang approaches to coupling reactions and hydrodynamics">

                <p class="caption">Two timesteps from a Castro
                  detonation calculation, showing the helium mass
                  fraction.  The orange points show the state in the
                  SDC evolution where each point represents substep
                  used in integrating the reaction network.  By
                  coupling the advection directly to the reaction
                  evolution, we see the evolution is smooth an
                  continuous.  In the Strang case, we see the
                  reactions in the first Δt/2 of the evolution take
                  the state far from the smooth SDC solution.
                  Advection at the midpoint in time over-corrects the
                  solution, and then the final Δt/2 of reaction brings
                  us back to the SDC solution.

              </section>

              <section>

                <a name="gpus"></a>

	        <header class="major">
		  <h2>Performance portability</h2>
	        </header>

                <p>All of our codes are written to be performance
                  portable%mdash;able to run on anything from a laptop to
                  a supercomputer.  Through
                  the <a href="https://amrex-codes.github.io/amrex/">AMReX</a>
                  library, we write our compute kernels in C++ and use
                  Parallel-For loops that loop over the zones in a grid.
                  On GPUs, each zone is assigned to a GPU thread, while on
                  CPUs, we use logical tiling and OpenMP to distribute the
                  work over processor codes.

                <p><img class="center" src="wdmerger_gpu.png" alt="GPU vs. CPU scaling for the WD merger problem">

                <p class="caption">Comparison of GPU (6 NVIDIA V100 /
                  node) and CPU (42 Power9 cores / node) on the OLCF
                  Summit machine for a merge white dwarf simulation
                  (hydrodynamics and full self-gravity).


	      </section>

	    </div>
	  </div>
	</div>
      </section>

      <!-- Footer -->
      <section id="footer">
	<div class="container">

	  <!-- Links -->
	  <section>
	    <div>
              Department of Physics & Astronomy &bull;
              Stony Brook University &bull; Stony Brook, NY 11794-3800
              <br>ESS 452 &bull; (631) 632-8225
              <br>michael.zingale@stonybrook.edu
              <p><span class="fab fa-youtube"></span>&nbsp; <a href="http://www.youtube.com/user/michaelzingale">http://www.youtube.com/user/michaelzingale</a>
                <br><i class="fab fa-github"></i>&nbsp; <a href="https://github.com/zingale">https://github.com/zingale</a></p>
	    </div>
	  </section>

	</div>
      </section>

      <!-- Copyright -->
      <div id="copyright">
	Design: <a href="http://html5up.net">HTML5 UP</a>
      </div>

    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>

  </body>
</html>
